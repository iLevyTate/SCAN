{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iLevyTate/SCAN/blob/main/STAC_Spiked_Transformer_Augmenting_Cognition_Smaller_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision transformers snntorch datasets -U bitsandbytes accelerate evaluate pytest"
      ],
      "metadata": {
        "id": "IzidOLemOuVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test_script.py\n",
        "import pytest\n",
        "import torch\n",
        "import torch.nn as nn  # Correct import for nn\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from datasets import load_dataset\n",
        "from torch.amp import autocast\n",
        "from torch.nn.utils import prune\n",
        "# Define the necessary classes and functions here (AdExNeuron, SNNLayer, CombinedModel, apply_model_quantization, apply_model_pruning)\n",
        "\n",
        "class AdExNeuron(nn.Module):\n",
        "    def __init__(self, input_size, output_size, tau_m=20.0, tau_w=100.0, a=0.001, b=0.05, V_th=-50.0, V_reset=-65.0):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.tau_m = tau_m\n",
        "        self.tau_w = tau_w\n",
        "        self.a = a\n",
        "        self.b = b\n",
        "        self.V_th = V_th\n",
        "        self.V_reset = V_reset\n",
        "        self.fc = nn.Linear(input_size, output_size)\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        batch_size = input_tensor.size(0)\n",
        "        device = input_tensor.device\n",
        "        if not hasattr(self, 'V') or self.V.size(0) != batch_size:\n",
        "            self.V = nn.Parameter(torch.ones(batch_size, self.output_size, device=device) * self.V_reset, requires_grad=False)\n",
        "            self.w = nn.Parameter(torch.zeros(batch_size, self.output_size, device=device), requires_grad=False)\n",
        "        I = self.fc(input_tensor)\n",
        "        dV = (I - self.w - (self.V - self.V_reset) / self.tau_m) / self.tau_m\n",
        "        dw = (self.a * (self.V - self.V_reset) - self.w) / self.tau_w\n",
        "        self.V.data += dV\n",
        "        self.w.data += dw\n",
        "        spikes = (self.V >= self.V_th).float()\n",
        "        self.V.data = self.V * (1 - spikes) + self.V_reset * spikes\n",
        "        self.w.data += self.b * spikes\n",
        "        return spikes\n",
        "\n",
        "class SNNLayer(nn.Module):\n",
        "    def __init__(self, input_size, output_size, num_recurrent_layers=1):\n",
        "        super(SNNLayer, self).__init__()\n",
        "        self.fc = nn.Linear(input_size, output_size)\n",
        "        self.adex = AdExNeuron(output_size, output_size)\n",
        "        self.recurrent_layers = nn.ModuleList([AdExNeuron(output_size, output_size) for _ in range(num_recurrent_layers)])\n",
        "        self.gate = nn.Sigmoid()\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "        self.batch_norm = nn.BatchNorm1d(output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        spk_out = []\n",
        "        for t in range(x.size(1)):\n",
        "            input_fc = self.fc(x[:, t])\n",
        "            if self.training and x.size(0) > 1:\n",
        "                input_fc = self.batch_norm(input_fc)\n",
        "            input_fc = self.dropout(input_fc)\n",
        "            spk = self.adex(input_fc)\n",
        "            for i, layer in enumerate(self.recurrent_layers):\n",
        "                recurrent_input = self.gate(spk) * input_fc\n",
        "                spk = layer(recurrent_input)\n",
        "            spk_out.append(spk)\n",
        "        return torch.stack(spk_out, dim=1)\n",
        "\n",
        "class CombinedModel(nn.Module):\n",
        "    def __init__(self, transformer_model, snn_output_size):\n",
        "        super(CombinedModel, self).__init__()\n",
        "        self.transformer = transformer_model\n",
        "        self.snn_layer = SNNLayer(self.transformer.config.hidden_size, snn_output_size)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        with autocast(device_type='cuda', enabled=torch.cuda.is_available()):\n",
        "            transformer_outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
        "            last_hidden_state = transformer_outputs.hidden_states[-1]\n",
        "            snn_outputs = self.snn_layer(last_hidden_state)\n",
        "        return snn_outputs\n",
        "\n",
        "def apply_model_quantization(model):\n",
        "    model.eval()\n",
        "    model.to('cpu')\n",
        "    def quantize_layer(layer):\n",
        "        if isinstance(layer, nn.Linear):\n",
        "            return torch.quantization.QuantWrapper(layer)\n",
        "        for name, child in layer.named_children():\n",
        "            layer.add_module(name, quantize_layer(child))\n",
        "        return layer\n",
        "    quantized_model = quantize_layer(model)\n",
        "    torch.quantization.prepare(quantized_model, inplace=True)\n",
        "    torch.quantization.convert(quantized_model, inplace=True)\n",
        "    return quantized_model\n",
        "\n",
        "def apply_model_pruning(model):\n",
        "    model.eval()\n",
        "    parameters_to_prune = []\n",
        "    for module in model.modules():\n",
        "        if isinstance(module, nn.Linear):\n",
        "            parameters_to_prune.append((module, 'weight'))\n",
        "    prune.global_unstructured(parameters_to_prune, pruning_method=prune.L1Unstructured, amount=0.4)\n",
        "    return model\n",
        "\n",
        "# Pytest Fixtures and Tests\n",
        "@pytest.fixture(scope=\"module\")\n",
        "def config():\n",
        "    return {\n",
        "        \"transformer_name\": \"gpt2\",\n",
        "        \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "        \"input_shape\": (2, 10, 768)\n",
        "    }\n",
        "\n",
        "@pytest.fixture(scope=\"module\")\n",
        "def tokenizer(config):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(config[\"transformer_name\"])\n",
        "    tokenizer.pad_token = tokenizer.eos_token  # Set pad token\n",
        "    return tokenizer\n",
        "\n",
        "@pytest.fixture(scope=\"module\")\n",
        "def transformer_model(config):\n",
        "    model = AutoModelForCausalLM.from_pretrained(config[\"transformer_name\"]).to(config[\"device\"])\n",
        "    return model\n",
        "\n",
        "@pytest.fixture(scope=\"module\")\n",
        "def snn_layer(config, transformer_model):\n",
        "    return SNNLayer(input_size=transformer_model.config.hidden_size, output_size=512).to(config[\"device\"])\n",
        "\n",
        "def test_snn_layer(config, snn_layer):\n",
        "    input_data = torch.zeros(config[\"input_shape\"], device=config[\"device\"])\n",
        "    with autocast(device_type='cuda', enabled=torch.cuda.is_available()):\n",
        "        output_data = snn_layer(input_data)\n",
        "    assert output_data.shape == (2, 10, 512)\n",
        "\n",
        "def test_combined_model(config, transformer_model):\n",
        "    combined_model = CombinedModel(\n",
        "        transformer_model=transformer_model,\n",
        "        snn_output_size=512\n",
        "    ).to(config[\"device\"])\n",
        "\n",
        "    input_ids = torch.zeros((2, 10), dtype=torch.long, device=config[\"device\"])\n",
        "    attention_mask = torch.ones((2, 10), dtype=torch.long, device=config[\"device\"])\n",
        "    with autocast(device_type='cuda', enabled=torch.cuda.is_available()):\n",
        "        output_data = combined_model(input_ids, attention_mask)\n",
        "    assert output_data.shape == (2, 10, 512)\n",
        "\n",
        "def test_data_loading_and_tokenization(tokenizer):\n",
        "    dataset = load_dataset(\"squad\", split=\"train[:1%]\")\n",
        "    assert len(dataset) > 0\n",
        "\n",
        "    def tokenize_dataset(examples):\n",
        "        return tokenizer(examples['context'], padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "    tokenized_dataset = dataset.map(tokenize_dataset, batched=True)\n",
        "    assert 'input_ids' in tokenized_dataset.features\n",
        "\n",
        "def test_inference_pipeline(config, tokenizer, transformer_model):\n",
        "    input_text = \"How does the prefrontal cortex handle decision-making?\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).to(config[\"device\"])\n",
        "\n",
        "    if transformer_model.config.pad_token_id is None:\n",
        "        transformer_model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "    transformer_model.eval()\n",
        "    with torch.no_grad():\n",
        "        with autocast(device_type='cuda', enabled=torch.cuda.is_available()):\n",
        "            outputs = transformer_model.generate(\n",
        "                input_ids=inputs['input_ids'],\n",
        "                attention_mask=inputs['attention_mask'],\n",
        "                max_length=50,\n",
        "                use_cache=True,\n",
        "                pad_token_id=tokenizer.pad_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    assert len(generated_text) > 0\n",
        "\n",
        "def test_model_quantization(config, transformer_model):\n",
        "    combined_model = CombinedModel(\n",
        "        transformer_model=transformer_model,\n",
        "        snn_output_size=512\n",
        "    ).to(config[\"device\"])\n",
        "\n",
        "    quantized_model = apply_model_quantization(combined_model)\n",
        "\n",
        "    input_text = \"How does the prefrontal cortex handle decision-making?\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(config[\"transformer_name\"])\n",
        "    tokenizer.pad_token = tokenizer.eos_token  # Set pad token\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).to('cpu')\n",
        "\n",
        "    quantized_model.to('cpu')  # Ensure the quantized model is on the CPU\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = quantized_model(inputs['input_ids'], inputs['attention_mask'])\n",
        "\n",
        "    assert outputs.shape == (1, len(inputs['input_ids'][0]), 512)\n",
        "\n",
        "def test_model_pruning(config, transformer_model):\n",
        "    combined_model = CombinedModel(\n",
        "        transformer_model=transformer_model,\n",
        "        snn_output_size=512\n",
        "    ).to(config[\"device\"])\n",
        "\n",
        "    pruned_model = apply_model_pruning(combined_model)\n",
        "    assert any(\"weight_orig\" not in name for name, _ in pruned_model.named_parameters())\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    pytest.main()\n"
      ],
      "metadata": {
        "id": "_CoHpk2b6GRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest -v test_script.py"
      ],
      "metadata": {
        "id": "-QssWVdNhRrO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "cell_execution_strategy": "setup",
      "gpuType": "L4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}